# ğŸ¤– Onscreen-Marketing-System AI

This is a powerful **FastAPI-based API** that evaluates student answers by comparing them with suggested answers using both **LLMs** (Gemini) and **embedding models** (SBERT, MiniLM, LaBSE). It returns an auto-generated score and feedback, useful for educational assessments, edtech platforms, or intelligent grading systems.

---

## ğŸš€ Features

- âœ… Uses **Gemini LLM** for scoring and one-line feedback
- âœ… Embedding-based similarity scoring with:
  - SBERT (`paraphrase-MiniLM-L6-v2`)
  - MiniLM (`paraphrase-MiniLM-L6-v2`)
  - LaBSE (`sentence-transformers/LaBSE`)
- âœ… Returns scores on a scale of 0â€“10 for each model
- âœ… FastAPI-powered endpoint for batch evaluation
- âœ… CORS enabled (useful for frontend testing)

---

## ğŸ“¦ Tech Stack

- **Python 3.11+**
- **FastAPI** for backend
- **Google Generative AI (Gemini)** for LLM evaluation
- **SentenceTransformers** for embedding-based models
- **Sklearn** for cosine similarity
- **Uvicorn** for ASGI server

---

## ğŸ§  How It Works

Each student answer is compared to the suggested answer using the following:

| Model        | Method                          | Output           |
|--------------|----------------------------------|------------------|
| Gemini       | LLM prompt + response            | Score + Feedback |
| SBERT        | Embedding similarity             | Score (0â€“10)     |
| MiniLM       | Embedding similarity             | Score (0â€“10)     |
| LaBSE        | Embedding similarity             | Score (0â€“10)     |

---

## ğŸ“ Project Structure

```

.
â”œâ”€â”€ main.py                    # Main FastAPI app with all models and endpoints
â”œâ”€â”€ requirements.txt           # Python dependencies
â””â”€â”€ .env                       # API key (Gemini)

````

---

## ğŸ§ª API Usage

### ğŸ”— POST `/evaluate`

Evaluate one or more student answers.

#### âœ… Request Body (JSON)
```json
[
  {
    "suggested_answer": "AI simulates human intelligence.",
    "student_answer": "AI makes machines think."
  }
]
````

#### âœ… Sample Response

```json
{
  "results": [
    {
      "suggested_answer": "AI simulates human intelligence.",
      "student_answer": "AI makes machines think.",
      "gemini_score": 8.5,
      "feedback": "Good answer but slightly lacks depth.",
      "sbert_score": 9.2,
      "minilm_score": 9.1,
      "labse_score": 8.8
    }
  ]
}
```

---

## â–¶ï¸ Running Locally

### 1ï¸âƒ£ Clone the repo

```bash
git clone https://github.com/Priya-Rathor/student-evaluation-api.git
cd student-evaluation-api
```

### 2ï¸âƒ£ Install dependencies

```bash
pip install -r requirements.txt
```

### 3ï¸âƒ£ Create `.env` file

```env
GOOGLE_API_KEY=your-gemini-api-key
```

> ğŸ” Or directly paste your Gemini API key in `genai.configure()` in `main.py`.

### 4ï¸âƒ£ Run the app

```bash
uvicorn main:app --reload --port 7100
```

---

## ğŸŒ Test the Endpoint

You can test it using:

* ğŸ”µ [Thunder Client](https://www.thunderclient.com/)
* ğŸŸ£ Postman
* ğŸ”¶ Curl
* ğŸŒ Your frontend via CORS

---

## ğŸ›¡ï¸ Error Handling

* If LLM or model fails, it gracefully returns `0.0` as score.
* Error logs are printed to the console for debugging.

---

## ğŸ§‘â€ğŸ’» Author

**Priya Rathor**
ğŸ”— [GitHub Profile](https://github.com/Priya-Rathor)

---

## ğŸ“Œ To-Do (Optional)

* [ ] Add MongoDB to store evaluation reports
* [ ] Add retry logic for Gemini API
* [ ] Add test cases using `pytest`
* [ ] Add frontend form for demo

---

